{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting scikit-learn\n",
      "  Downloading scikit_learn-1.6.1-cp313-cp313-win_amd64.whl.metadata (15 kB)\n",
      "Requirement already satisfied: numpy>=1.19.5 in c:\\users\\haris\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from scikit-learn) (2.2.3)\n",
      "Collecting scipy>=1.6.0 (from scikit-learn)\n",
      "  Downloading scipy-1.15.2-cp313-cp313-win_amd64.whl.metadata (60 kB)\n",
      "Collecting joblib>=1.2.0 (from scikit-learn)\n",
      "  Downloading joblib-1.4.2-py3-none-any.whl.metadata (5.4 kB)\n",
      "Collecting threadpoolctl>=3.1.0 (from scikit-learn)\n",
      "  Downloading threadpoolctl-3.5.0-py3-none-any.whl.metadata (13 kB)\n",
      "Downloading scikit_learn-1.6.1-cp313-cp313-win_amd64.whl (11.1 MB)\n",
      "   ---------------------------------------- 0.0/11.1 MB ? eta -:--:--\n",
      "   ---------------------------------------  11.0/11.1 MB 59.3 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 11.1/11.1 MB 53.6 MB/s eta 0:00:00\n",
      "Downloading joblib-1.4.2-py3-none-any.whl (301 kB)\n",
      "Downloading scipy-1.15.2-cp313-cp313-win_amd64.whl (41.0 MB)\n",
      "   ---------------------------------------- 0.0/41.0 MB ? eta -:--:--\n",
      "   -------------- ------------------------- 14.7/41.0 MB 73.9 MB/s eta 0:00:01\n",
      "   ----------------------------- ---------- 29.9/41.0 MB 73.7 MB/s eta 0:00:01\n",
      "   ---------------------------------------  40.9/41.0 MB 73.7 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 41.0/41.0 MB 59.3 MB/s eta 0:00:00\n",
      "Downloading threadpoolctl-3.5.0-py3-none-any.whl (18 kB)\n",
      "Installing collected packages: threadpoolctl, scipy, joblib, scikit-learn\n",
      "Successfully installed joblib-1.4.2 scikit-learn-1.6.1 scipy-1.15.2 threadpoolctl-3.5.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.3.1 -> 25.0.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "pip install scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas in c:\\users\\haris\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (2.2.3)\n",
      "Collecting torch\n",
      "  Downloading torch-2.6.0-cp313-cp313-win_amd64.whl.metadata (28 kB)\n",
      "Collecting transformers\n",
      "  Downloading transformers-4.49.0-py3-none-any.whl.metadata (44 kB)\n",
      "Requirement already satisfied: numpy>=1.26.0 in c:\\users\\haris\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from pandas) (2.2.3)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\haris\\appdata\\roaming\\python\\python313\\site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\haris\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from pandas) (2025.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\haris\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from pandas) (2025.1)\n",
      "Collecting filelock (from torch)\n",
      "  Downloading filelock-3.17.0-py3-none-any.whl.metadata (2.9 kB)\n",
      "Collecting typing-extensions>=4.10.0 (from torch)\n",
      "  Downloading typing_extensions-4.12.2-py3-none-any.whl.metadata (3.0 kB)\n",
      "Collecting networkx (from torch)\n",
      "  Downloading networkx-3.4.2-py3-none-any.whl.metadata (6.3 kB)\n",
      "Collecting jinja2 (from torch)\n",
      "  Downloading jinja2-3.1.5-py3-none-any.whl.metadata (2.6 kB)\n",
      "Collecting fsspec (from torch)\n",
      "  Downloading fsspec-2025.2.0-py3-none-any.whl.metadata (11 kB)\n",
      "Collecting setuptools (from torch)\n",
      "  Downloading setuptools-75.8.0-py3-none-any.whl.metadata (6.7 kB)\n",
      "Collecting sympy==1.13.1 (from torch)\n",
      "  Downloading sympy-1.13.1-py3-none-any.whl.metadata (12 kB)\n",
      "Collecting mpmath<1.4,>=1.1.0 (from sympy==1.13.1->torch)\n",
      "  Downloading mpmath-1.3.0-py3-none-any.whl.metadata (8.6 kB)\n",
      "Collecting huggingface-hub<1.0,>=0.26.0 (from transformers)\n",
      "  Downloading huggingface_hub-0.28.1-py3-none-any.whl.metadata (13 kB)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\haris\\appdata\\roaming\\python\\python313\\site-packages (from transformers) (24.2)\n",
      "Collecting pyyaml>=5.1 (from transformers)\n",
      "  Downloading PyYAML-6.0.2-cp313-cp313-win_amd64.whl.metadata (2.1 kB)\n",
      "Collecting regex!=2019.12.17 (from transformers)\n",
      "  Downloading regex-2024.11.6-cp313-cp313-win_amd64.whl.metadata (41 kB)\n",
      "Collecting requests (from transformers)\n",
      "  Downloading requests-2.32.3-py3-none-any.whl.metadata (4.6 kB)\n",
      "Collecting tokenizers<0.22,>=0.21 (from transformers)\n",
      "  Downloading tokenizers-0.21.0-cp39-abi3-win_amd64.whl.metadata (6.9 kB)\n",
      "Collecting safetensors>=0.4.1 (from transformers)\n",
      "  Downloading safetensors-0.5.2-cp38-abi3-win_amd64.whl.metadata (3.9 kB)\n",
      "Collecting tqdm>=4.27 (from transformers)\n",
      "  Downloading tqdm-4.67.1-py3-none-any.whl.metadata (57 kB)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\haris\\appdata\\roaming\\python\\python313\\site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\haris\\appdata\\roaming\\python\\python313\\site-packages (from tqdm>=4.27->transformers) (0.4.6)\n",
      "Collecting MarkupSafe>=2.0 (from jinja2->torch)\n",
      "  Downloading MarkupSafe-3.0.2-cp313-cp313-win_amd64.whl.metadata (4.1 kB)\n",
      "Collecting charset-normalizer<4,>=2 (from requests->transformers)\n",
      "  Downloading charset_normalizer-3.4.1-cp313-cp313-win_amd64.whl.metadata (36 kB)\n",
      "Collecting idna<4,>=2.5 (from requests->transformers)\n",
      "  Downloading idna-3.10-py3-none-any.whl.metadata (10 kB)\n",
      "Collecting urllib3<3,>=1.21.1 (from requests->transformers)\n",
      "  Downloading urllib3-2.3.0-py3-none-any.whl.metadata (6.5 kB)\n",
      "Collecting certifi>=2017.4.17 (from requests->transformers)\n",
      "  Downloading certifi-2025.1.31-py3-none-any.whl.metadata (2.5 kB)\n",
      "Downloading torch-2.6.0-cp313-cp313-win_amd64.whl (204.1 MB)\n",
      "   ---------------------------------------- 0.0/204.1 MB ? eta -:--:--\n",
      "   - -------------------------------------- 7.9/204.1 MB 40.3 MB/s eta 0:00:05\n",
      "   ---- ----------------------------------- 22.5/204.1 MB 57.4 MB/s eta 0:00:04\n",
      "   ------- -------------------------------- 38.5/204.1 MB 63.3 MB/s eta 0:00:03\n",
      "   ---------- ----------------------------- 53.5/204.1 MB 65.9 MB/s eta 0:00:03\n",
      "   ------------ --------------------------- 62.9/204.1 MB 62.6 MB/s eta 0:00:03\n",
      "   --------------- ------------------------ 79.2/204.1 MB 63.7 MB/s eta 0:00:02\n",
      "   ------------------ --------------------- 94.1/204.1 MB 65.2 MB/s eta 0:00:02\n",
      "   --------------------- ----------------- 110.1/204.1 MB 66.3 MB/s eta 0:00:02\n",
      "   ----------------------- --------------- 125.0/204.1 MB 67.1 MB/s eta 0:00:02\n",
      "   -------------------------- ------------ 140.8/204.1 MB 67.7 MB/s eta 0:00:01\n",
      "   ----------------------------- --------- 156.5/204.1 MB 68.3 MB/s eta 0:00:01\n",
      "   -------------------------------- ------ 171.4/204.1 MB 68.7 MB/s eta 0:00:01\n",
      "   ----------------------------------- --- 186.4/204.1 MB 69.1 MB/s eta 0:00:01\n",
      "   --------------------------------------  199.0/204.1 MB 68.6 MB/s eta 0:00:01\n",
      "   --------------------------------------  203.9/204.1 MB 68.6 MB/s eta 0:00:01\n",
      "   --------------------------------------  203.9/204.1 MB 68.6 MB/s eta 0:00:01\n",
      "   --------------------------------------- 204.1/204.1 MB 58.5 MB/s eta 0:00:00\n",
      "Downloading sympy-1.13.1-py3-none-any.whl (6.2 MB)\n",
      "   ---------------------------------------- 0.0/6.2 MB ? eta -:--:--\n",
      "   ---------------------------------------- 6.2/6.2 MB 56.7 MB/s eta 0:00:00\n",
      "Downloading transformers-4.49.0-py3-none-any.whl (10.0 MB)\n",
      "   ---------------------------------------- 0.0/10.0 MB ? eta -:--:--\n",
      "   ---------------------------------------- 10.0/10.0 MB 56.5 MB/s eta 0:00:00\n",
      "Downloading huggingface_hub-0.28.1-py3-none-any.whl (464 kB)\n",
      "Downloading fsspec-2025.2.0-py3-none-any.whl (184 kB)\n",
      "Downloading PyYAML-6.0.2-cp313-cp313-win_amd64.whl (156 kB)\n",
      "Downloading regex-2024.11.6-cp313-cp313-win_amd64.whl (273 kB)\n",
      "Downloading safetensors-0.5.2-cp38-abi3-win_amd64.whl (303 kB)\n",
      "Downloading tokenizers-0.21.0-cp39-abi3-win_amd64.whl (2.4 MB)\n",
      "   ---------------------------------------- 0.0/2.4 MB ? eta -:--:--\n",
      "   ---------------------------------------- 2.4/2.4 MB 54.0 MB/s eta 0:00:00\n",
      "Downloading tqdm-4.67.1-py3-none-any.whl (78 kB)\n",
      "Downloading typing_extensions-4.12.2-py3-none-any.whl (37 kB)\n",
      "Downloading filelock-3.17.0-py3-none-any.whl (16 kB)\n",
      "Downloading jinja2-3.1.5-py3-none-any.whl (134 kB)\n",
      "Downloading networkx-3.4.2-py3-none-any.whl (1.7 MB)\n",
      "   ---------------------------------------- 0.0/1.7 MB ? eta -:--:--\n",
      "   ---------------------------------------- 1.7/1.7 MB 47.2 MB/s eta 0:00:00\n",
      "Downloading requests-2.32.3-py3-none-any.whl (64 kB)\n",
      "Downloading setuptools-75.8.0-py3-none-any.whl (1.2 MB)\n",
      "   ---------------------------------------- 0.0/1.2 MB ? eta -:--:--\n",
      "   ---------------------------------------- 1.2/1.2 MB 47.6 MB/s eta 0:00:00\n",
      "Downloading certifi-2025.1.31-py3-none-any.whl (166 kB)\n",
      "Downloading charset_normalizer-3.4.1-cp313-cp313-win_amd64.whl (102 kB)\n",
      "Downloading idna-3.10-py3-none-any.whl (70 kB)\n",
      "Downloading MarkupSafe-3.0.2-cp313-cp313-win_amd64.whl (15 kB)\n",
      "Downloading mpmath-1.3.0-py3-none-any.whl (536 kB)\n",
      "   ---------------------------------------- 0.0/536.2 kB ? eta -:--:--\n",
      "   --------------------------------------- 536.2/536.2 kB 30.7 MB/s eta 0:00:00\n",
      "Downloading urllib3-2.3.0-py3-none-any.whl (128 kB)\n",
      "Installing collected packages: mpmath, urllib3, typing-extensions, tqdm, sympy, setuptools, safetensors, regex, pyyaml, networkx, MarkupSafe, idna, fsspec, filelock, charset-normalizer, certifi, requests, jinja2, torch, huggingface-hub, tokenizers, transformers\n",
      "Successfully installed MarkupSafe-3.0.2 certifi-2025.1.31 charset-normalizer-3.4.1 filelock-3.17.0 fsspec-2025.2.0 huggingface-hub-0.28.1 idna-3.10 jinja2-3.1.5 mpmath-1.3.0 networkx-3.4.2 pyyaml-6.0.2 regex-2024.11.6 requests-2.32.3 safetensors-0.5.2 setuptools-75.8.0 sympy-1.13.1 tokenizers-0.21.0 torch-2.6.0 tqdm-4.67.1 transformers-4.49.0 typing-extensions-4.12.2 urllib3-2.3.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  WARNING: The script tqdm.exe is installed in 'c:\\Users\\haris\\AppData\\Local\\Programs\\Python\\Python313\\Scripts' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\n",
      "  WARNING: The script isympy.exe is installed in 'c:\\Users\\haris\\AppData\\Local\\Programs\\Python\\Python313\\Scripts' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\n",
      "  WARNING: The script normalizer.exe is installed in 'c:\\Users\\haris\\AppData\\Local\\Programs\\Python\\Python313\\Scripts' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\n",
      "  WARNING: The scripts torchfrtrace.exe and torchrun.exe are installed in 'c:\\Users\\haris\\AppData\\Local\\Programs\\Python\\Python313\\Scripts' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\n",
      "  WARNING: The script huggingface-cli.exe is installed in 'c:\\Users\\haris\\AppData\\Local\\Programs\\Python\\Python313\\Scripts' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\n",
      "  WARNING: The script transformers-cli.exe is installed in 'c:\\Users\\haris\\AppData\\Local\\Programs\\Python\\Python313\\Scripts' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\n",
      "\n",
      "[notice] A new release of pip is available: 24.3.1 -> 25.0.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "pip install pandas transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://download.pytorch.org/whl/cu126\n",
      "Requirement already satisfied: torch in c:\\users\\haris\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (2.6.0)\n",
      "Collecting torchvision\n",
      "  Downloading https://download.pytorch.org/whl/cu126/torchvision-0.21.0%2Bcu126-cp313-cp313-win_amd64.whl.metadata (6.3 kB)\n",
      "Collecting torchaudio\n",
      "  Downloading https://download.pytorch.org/whl/cu126/torchaudio-2.6.0%2Bcu126-cp313-cp313-win_amd64.whl.metadata (6.8 kB)\n",
      "Requirement already satisfied: filelock in c:\\users\\haris\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from torch) (3.17.0)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in c:\\users\\haris\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from torch) (4.12.2)\n",
      "Requirement already satisfied: networkx in c:\\users\\haris\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from torch) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\haris\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from torch) (3.1.5)\n",
      "Requirement already satisfied: fsspec in c:\\users\\haris\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from torch) (2025.2.0)\n",
      "Requirement already satisfied: setuptools in c:\\users\\haris\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from torch) (75.8.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in c:\\users\\haris\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from torch) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\haris\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from sympy==1.13.1->torch) (1.3.0)\n",
      "Requirement already satisfied: numpy in c:\\users\\haris\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from torchvision) (2.2.3)\n",
      "Collecting torch\n",
      "  Downloading https://download.pytorch.org/whl/cu126/torch-2.6.0%2Bcu126-cp313-cp313-win_amd64.whl.metadata (28 kB)\n",
      "Collecting pillow!=8.3.*,>=5.3.0 (from torchvision)\n",
      "  Downloading https://download.pytorch.org/whl/pillow-11.0.0-cp313-cp313-win_amd64.whl.metadata (9.3 kB)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\haris\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from jinja2->torch) (3.0.2)\n",
      "Downloading https://download.pytorch.org/whl/cu126/torchvision-0.21.0%2Bcu126-cp313-cp313-win_amd64.whl (6.1 MB)\n",
      "   ---------------------------------------- 0.0/6.1 MB ? eta -:--:--\n",
      "   ---------------------------------------- 6.1/6.1 MB 45.9 MB/s eta 0:00:00\n",
      "Downloading https://download.pytorch.org/whl/cu126/torch-2.6.0%2Bcu126-cp313-cp313-win_amd64.whl (2496.1 MB)\n",
      "   ---------------------------------------- 0.0/2.5 GB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/2.5 GB 71.7 MB/s eta 0:00:35\n",
      "   ---------------------------------------- 0.0/2.5 GB 72.5 MB/s eta 0:00:35\n",
      "    --------------------------------------- 0.0/2.5 GB 72.7 MB/s eta 0:00:34\n",
      "    --------------------------------------- 0.1/2.5 GB 72.8 MB/s eta 0:00:34\n",
      "   - -------------------------------------- 0.1/2.5 GB 72.9 MB/s eta 0:00:34\n",
      "   - -------------------------------------- 0.1/2.5 GB 72.9 MB/s eta 0:00:33\n",
      "   - -------------------------------------- 0.1/2.5 GB 72.9 MB/s eta 0:00:33\n",
      "   - -------------------------------------- 0.1/2.5 GB 72.9 MB/s eta 0:00:33\n",
      "   -- ------------------------------------- 0.1/2.5 GB 72.9 MB/s eta 0:00:33\n",
      "   -- ------------------------------------- 0.1/2.5 GB 72.9 MB/s eta 0:00:33\n",
      "   -- ------------------------------------- 0.2/2.5 GB 72.9 MB/s eta 0:00:32\n",
      "   -- ------------------------------------- 0.2/2.5 GB 73.0 MB/s eta 0:00:32\n",
      "   --- ------------------------------------ 0.2/2.5 GB 73.0 MB/s eta 0:00:32\n",
      "   --- ------------------------------------ 0.2/2.5 GB 73.0 MB/s eta 0:00:32\n",
      "   --- ------------------------------------ 0.2/2.5 GB 73.0 MB/s eta 0:00:32\n",
      "   --- ------------------------------------ 0.2/2.5 GB 73.0 MB/s eta 0:00:31\n",
      "   ---- ----------------------------------- 0.3/2.5 GB 73.0 MB/s eta 0:00:31\n",
      "   ---- ----------------------------------- 0.3/2.5 GB 73.0 MB/s eta 0:00:31\n",
      "   ---- ----------------------------------- 0.3/2.5 GB 73.1 MB/s eta 0:00:31\n",
      "   ---- ----------------------------------- 0.3/2.5 GB 73.1 MB/s eta 0:00:31\n",
      "   ----- ---------------------------------- 0.3/2.5 GB 73.1 MB/s eta 0:00:30\n",
      "   ----- ---------------------------------- 0.3/2.5 GB 73.1 MB/s eta 0:00:30\n",
      "   ----- ---------------------------------- 0.3/2.5 GB 73.1 MB/s eta 0:00:30\n",
      "   ----- ---------------------------------- 0.4/2.5 GB 73.1 MB/s eta 0:00:30\n",
      "   ----- ---------------------------------- 0.4/2.5 GB 73.2 MB/s eta 0:00:29\n",
      "   ------ --------------------------------- 0.4/2.5 GB 73.2 MB/s eta 0:00:29\n",
      "   ------ --------------------------------- 0.4/2.5 GB 73.2 MB/s eta 0:00:29\n",
      "   ------ --------------------------------- 0.4/2.5 GB 73.2 MB/s eta 0:00:29\n",
      "   ------ --------------------------------- 0.4/2.5 GB 73.2 MB/s eta 0:00:29\n",
      "   ------- -------------------------------- 0.4/2.5 GB 73.2 MB/s eta 0:00:28\n",
      "   ------- -------------------------------- 0.5/2.5 GB 73.1 MB/s eta 0:00:28\n",
      "   ------- -------------------------------- 0.5/2.5 GB 73.2 MB/s eta 0:00:28\n",
      "   ------- -------------------------------- 0.5/2.5 GB 73.2 MB/s eta 0:00:28\n",
      "   -------- ------------------------------- 0.5/2.5 GB 73.1 MB/s eta 0:00:28\n",
      "   -------- ------------------------------- 0.5/2.5 GB 72.9 MB/s eta 0:00:28\n",
      "   -------- ------------------------------- 0.5/2.5 GB 72.8 MB/s eta 0:00:27\n",
      "   -------- ------------------------------- 0.6/2.5 GB 72.8 MB/s eta 0:00:27\n",
      "   --------- ------------------------------ 0.6/2.5 GB 72.8 MB/s eta 0:00:27\n",
      "   --------- ------------------------------ 0.6/2.5 GB 72.8 MB/s eta 0:00:27\n",
      "   --------- ------------------------------ 0.6/2.5 GB 72.8 MB/s eta 0:00:27\n",
      "   --------- ------------------------------ 0.6/2.5 GB 72.8 MB/s eta 0:00:26\n",
      "   ---------- ----------------------------- 0.6/2.5 GB 72.8 MB/s eta 0:00:26\n",
      "   ---------- ----------------------------- 0.6/2.5 GB 72.8 MB/s eta 0:00:26\n",
      "   ---------- ----------------------------- 0.7/2.5 GB 72.8 MB/s eta 0:00:26\n",
      "   ---------- ----------------------------- 0.7/2.5 GB 72.8 MB/s eta 0:00:26\n",
      "   ---------- ----------------------------- 0.7/2.5 GB 72.8 MB/s eta 0:00:25\n",
      "   ----------- ---------------------------- 0.7/2.5 GB 72.8 MB/s eta 0:00:25\n",
      "   ----------- ---------------------------- 0.7/2.5 GB 72.8 MB/s eta 0:00:25\n",
      "   ----------- ---------------------------- 0.7/2.5 GB 71.6 MB/s eta 0:00:25\n",
      "   ----------- ---------------------------- 0.7/2.5 GB 68.5 MB/s eta 0:00:26\n",
      "   ----------- ---------------------------- 0.7/2.5 GB 68.5 MB/s eta 0:00:26\n",
      "   ------------ --------------------------- 0.8/2.5 GB 68.5 MB/s eta 0:00:26\n",
      "   ------------ --------------------------- 0.8/2.5 GB 68.6 MB/s eta 0:00:26\n",
      "   ------------ --------------------------- 0.8/2.5 GB 68.7 MB/s eta 0:00:25\n",
      "   ------------ --------------------------- 0.8/2.5 GB 68.8 MB/s eta 0:00:25\n",
      "   ------------- -------------------------- 0.8/2.5 GB 68.8 MB/s eta 0:00:25\n",
      "   ------------- -------------------------- 0.8/2.5 GB 68.8 MB/s eta 0:00:25\n",
      "   ------------- -------------------------- 0.8/2.5 GB 68.8 MB/s eta 0:00:24\n",
      "   ------------- -------------------------- 0.9/2.5 GB 68.8 MB/s eta 0:00:24\n",
      "   -------------- ------------------------- 0.9/2.5 GB 68.8 MB/s eta 0:00:24\n",
      "   -------------- ------------------------- 0.9/2.5 GB 68.8 MB/s eta 0:00:24\n",
      "   -------------- ------------------------- 0.9/2.5 GB 68.8 MB/s eta 0:00:24\n",
      "   -------------- ------------------------- 0.9/2.5 GB 68.8 MB/s eta 0:00:23\n",
      "   --------------- ------------------------ 0.9/2.5 GB 68.8 MB/s eta 0:00:23\n",
      "   --------------- ------------------------ 1.0/2.5 GB 68.8 MB/s eta 0:00:23\n",
      "   --------------- ------------------------ 1.0/2.5 GB 68.8 MB/s eta 0:00:23\n",
      "   --------------- ------------------------ 1.0/2.5 GB 68.8 MB/s eta 0:00:22\n",
      "   ---------------- ----------------------- 1.0/2.5 GB 73.2 MB/s eta 0:00:21\n",
      "   ---------------- ----------------------- 1.0/2.5 GB 73.1 MB/s eta 0:00:21\n",
      "   ---------------- ----------------------- 1.0/2.5 GB 73.2 MB/s eta 0:00:21\n",
      "   ---------------- ----------------------- 1.0/2.5 GB 73.1 MB/s eta 0:00:20\n",
      "   ---------------- ----------------------- 1.1/2.5 GB 73.0 MB/s eta 0:00:20\n",
      "   ----------------- ---------------------- 1.1/2.5 GB 72.9 MB/s eta 0:00:20\n",
      "   ----------------- ---------------------- 1.1/2.5 GB 72.8 MB/s eta 0:00:20\n",
      "   ----------------- ---------------------- 1.1/2.5 GB 72.9 MB/s eta 0:00:20\n",
      "   ----------------- ---------------------- 1.1/2.5 GB 72.8 MB/s eta 0:00:19\n",
      "   ------------------ --------------------- 1.1/2.5 GB 72.8 MB/s eta 0:00:19\n",
      "   ------------------ --------------------- 1.1/2.5 GB 72.9 MB/s eta 0:00:19\n",
      "   ------------------ --------------------- 1.2/2.5 GB 72.9 MB/s eta 0:00:19\n",
      "   ------------------ --------------------- 1.2/2.5 GB 72.9 MB/s eta 0:00:19\n",
      "   ------------------- -------------------- 1.2/2.5 GB 72.9 MB/s eta 0:00:18\n",
      "   ------------------- -------------------- 1.2/2.5 GB 72.9 MB/s eta 0:00:18\n",
      "   ------------------- -------------------- 1.2/2.5 GB 72.9 MB/s eta 0:00:18\n",
      "   ------------------- -------------------- 1.2/2.5 GB 72.9 MB/s eta 0:00:18\n",
      "   -------------------- ------------------- 1.3/2.5 GB 72.9 MB/s eta 0:00:18\n",
      "   -------------------- ------------------- 1.3/2.5 GB 72.9 MB/s eta 0:00:17\n",
      "   -------------------- ------------------- 1.3/2.5 GB 72.9 MB/s eta 0:00:17\n",
      "   -------------------- ------------------- 1.3/2.5 GB 72.9 MB/s eta 0:00:17\n",
      "   -------------------- ------------------- 1.3/2.5 GB 72.9 MB/s eta 0:00:17\n",
      "   --------------------- ------------------ 1.3/2.5 GB 73.0 MB/s eta 0:00:17\n",
      "   --------------------- ------------------ 1.3/2.5 GB 73.1 MB/s eta 0:00:16\n",
      "   --------------------- ------------------ 1.4/2.5 GB 73.0 MB/s eta 0:00:16\n",
      "   --------------------- ------------------ 1.4/2.5 GB 72.9 MB/s eta 0:00:16\n",
      "   ---------------------- ----------------- 1.4/2.5 GB 72.8 MB/s eta 0:00:16\n",
      "   ---------------------- ----------------- 1.4/2.5 GB 72.8 MB/s eta 0:00:16\n",
      "   ---------------------- ----------------- 1.4/2.5 GB 72.8 MB/s eta 0:00:15\n",
      "   ---------------------- ----------------- 1.4/2.5 GB 72.8 MB/s eta 0:00:15\n",
      "   ----------------------- ---------------- 1.4/2.5 GB 72.8 MB/s eta 0:00:15\n",
      "   ----------------------- ---------------- 1.5/2.5 GB 72.8 MB/s eta 0:00:15\n",
      "   ----------------------- ---------------- 1.5/2.5 GB 68.6 MB/s eta 0:00:16\n",
      "   ----------------------- ---------------- 1.5/2.5 GB 68.6 MB/s eta 0:00:15\n",
      "   ----------------------- ---------------- 1.5/2.5 GB 68.6 MB/s eta 0:00:15\n",
      "   ------------------------ --------------- 1.5/2.5 GB 68.6 MB/s eta 0:00:15\n",
      "   ------------------------ --------------- 1.5/2.5 GB 68.6 MB/s eta 0:00:15\n",
      "   ------------------------ --------------- 1.5/2.5 GB 68.6 MB/s eta 0:00:15\n",
      "   ------------------------ --------------- 1.5/2.5 GB 68.6 MB/s eta 0:00:14\n",
      "   ------------------------- -------------- 1.6/2.5 GB 68.6 MB/s eta 0:00:14\n",
      "   ------------------------- -------------- 1.6/2.5 GB 68.6 MB/s eta 0:00:14\n",
      "   ------------------------- -------------- 1.6/2.5 GB 68.6 MB/s eta 0:00:14\n",
      "   ------------------------- -------------- 1.6/2.5 GB 68.6 MB/s eta 0:00:13\n",
      "   ------------------------- -------------- 1.6/2.5 GB 68.7 MB/s eta 0:00:13\n",
      "   -------------------------- ------------- 1.6/2.5 GB 68.8 MB/s eta 0:00:13\n",
      "   -------------------------- ------------- 1.6/2.5 GB 68.9 MB/s eta 0:00:13\n",
      "   -------------------------- ------------- 1.7/2.5 GB 68.9 MB/s eta 0:00:13\n",
      "   -------------------------- ------------- 1.7/2.5 GB 68.9 MB/s eta 0:00:12\n",
      "   --------------------------- ------------ 1.7/2.5 GB 68.9 MB/s eta 0:00:12\n",
      "   --------------------------- ------------ 1.7/2.5 GB 68.9 MB/s eta 0:00:12\n",
      "   --------------------------- ------------ 1.7/2.5 GB 73.1 MB/s eta 0:00:11\n",
      "   --------------------------- ------------ 1.7/2.5 GB 73.1 MB/s eta 0:00:11\n",
      "   ---------------------------- ----------- 1.8/2.5 GB 73.1 MB/s eta 0:00:11\n",
      "   ---------------------------- ----------- 1.8/2.5 GB 73.1 MB/s eta 0:00:10\n",
      "   ---------------------------- ----------- 1.8/2.5 GB 73.1 MB/s eta 0:00:10\n",
      "   ---------------------------- ----------- 1.8/2.5 GB 73.1 MB/s eta 0:00:10\n",
      "   ----------------------------- ---------- 1.8/2.5 GB 73.1 MB/s eta 0:00:10\n",
      "   ----------------------------- ---------- 1.8/2.5 GB 73.1 MB/s eta 0:00:10\n",
      "   ----------------------------- ---------- 1.8/2.5 GB 73.1 MB/s eta 0:00:09\n",
      "   ----------------------------- ---------- 1.9/2.5 GB 73.1 MB/s eta 0:00:09\n",
      "   ------------------------------ --------- 1.9/2.5 GB 73.1 MB/s eta 0:00:09\n",
      "   ------------------------------ --------- 1.9/2.5 GB 73.1 MB/s eta 0:00:09\n",
      "   ------------------------------ --------- 1.9/2.5 GB 73.1 MB/s eta 0:00:09\n",
      "   ------------------------------ --------- 1.9/2.5 GB 73.1 MB/s eta 0:00:08\n",
      "   ------------------------------ --------- 1.9/2.5 GB 73.1 MB/s eta 0:00:08\n",
      "   ------------------------------- -------- 1.9/2.5 GB 73.2 MB/s eta 0:00:08\n",
      "   ------------------------------- -------- 2.0/2.5 GB 73.2 MB/s eta 0:00:08\n",
      "   ------------------------------- -------- 2.0/2.5 GB 73.2 MB/s eta 0:00:08\n",
      "   ------------------------------- -------- 2.0/2.5 GB 73.0 MB/s eta 0:00:07\n",
      "   -------------------------------- ------- 2.0/2.5 GB 73.0 MB/s eta 0:00:07\n",
      "   -------------------------------- ------- 2.0/2.5 GB 72.9 MB/s eta 0:00:07\n",
      "   -------------------------------- ------- 2.0/2.5 GB 72.8 MB/s eta 0:00:07\n",
      "   -------------------------------- ------- 2.1/2.5 GB 72.7 MB/s eta 0:00:07\n",
      "   --------------------------------- ------ 2.1/2.5 GB 72.7 MB/s eta 0:00:06\n",
      "   --------------------------------- ------ 2.1/2.5 GB 72.7 MB/s eta 0:00:06\n",
      "   --------------------------------- ------ 2.1/2.5 GB 72.7 MB/s eta 0:00:06\n",
      "   --------------------------------- ------ 2.1/2.5 GB 72.7 MB/s eta 0:00:06\n",
      "   ---------------------------------- ----- 2.1/2.5 GB 72.7 MB/s eta 0:00:06\n",
      "   ---------------------------------- ----- 2.1/2.5 GB 72.7 MB/s eta 0:00:05\n",
      "   ---------------------------------- ----- 2.2/2.5 GB 72.7 MB/s eta 0:00:05\n",
      "   ---------------------------------- ----- 2.2/2.5 GB 72.7 MB/s eta 0:00:05\n",
      "   ----------------------------------- ---- 2.2/2.5 GB 72.7 MB/s eta 0:00:05\n",
      "   ----------------------------------- ---- 2.2/2.5 GB 72.7 MB/s eta 0:00:05\n",
      "   ----------------------------------- ---- 2.2/2.5 GB 68.4 MB/s eta 0:00:05\n",
      "   ----------------------------------- ---- 2.2/2.5 GB 68.4 MB/s eta 0:00:05\n",
      "   ----------------------------------- ---- 2.2/2.5 GB 68.4 MB/s eta 0:00:04\n",
      "   ----------------------------------- ---- 2.2/2.5 GB 68.4 MB/s eta 0:00:04\n",
      "   ------------------------------------ --- 2.3/2.5 GB 68.5 MB/s eta 0:00:04\n",
      "   ------------------------------------ --- 2.3/2.5 GB 68.6 MB/s eta 0:00:04\n",
      "   ------------------------------------ --- 2.3/2.5 GB 68.6 MB/s eta 0:00:04\n",
      "   ------------------------------------ --- 2.3/2.5 GB 68.7 MB/s eta 0:00:03\n",
      "   ------------------------------------- -- 2.3/2.5 GB 68.7 MB/s eta 0:00:03\n",
      "   ------------------------------------- -- 2.3/2.5 GB 68.7 MB/s eta 0:00:03\n",
      "   ------------------------------------- -- 2.3/2.5 GB 68.7 MB/s eta 0:00:03\n",
      "   ------------------------------------- -- 2.4/2.5 GB 68.7 MB/s eta 0:00:02\n",
      "   -------------------------------------- - 2.4/2.5 GB 68.7 MB/s eta 0:00:02\n",
      "   -------------------------------------- - 2.4/2.5 GB 68.7 MB/s eta 0:00:02\n",
      "   -------------------------------------- - 2.4/2.5 GB 68.7 MB/s eta 0:00:02\n",
      "   -------------------------------------- - 2.4/2.5 GB 68.7 MB/s eta 0:00:02\n",
      "   ---------------------------------------  2.4/2.5 GB 68.7 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.5/2.5 GB 68.7 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.5/2.5 GB 73.1 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.5/2.5 GB 73.1 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.5/2.5 GB 73.1 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.5/2.5 GB 73.1 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.5/2.5 GB 73.1 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.5/2.5 GB 73.1 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.5/2.5 GB 73.1 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.5/2.5 GB 73.1 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.5/2.5 GB 73.1 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.5/2.5 GB 73.1 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.5/2.5 GB 73.1 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.5/2.5 GB 73.1 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.5/2.5 GB 73.1 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.5/2.5 GB 73.1 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.5/2.5 GB 73.1 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.5/2.5 GB 73.1 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.5/2.5 GB 73.1 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.5/2.5 GB 73.1 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.5/2.5 GB 73.1 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.5/2.5 GB 73.1 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.5/2.5 GB 73.1 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.5/2.5 GB 73.1 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.5/2.5 GB 73.1 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.5/2.5 GB 73.1 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.5/2.5 GB 73.1 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.5/2.5 GB 73.1 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.5/2.5 GB 73.1 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.5/2.5 GB 73.1 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.5/2.5 GB 73.1 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.5/2.5 GB 73.1 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.5/2.5 GB 73.1 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.5/2.5 GB 73.1 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.5/2.5 GB 73.1 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.5/2.5 GB 73.1 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.5/2.5 GB 73.1 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.5/2.5 GB 73.1 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.5/2.5 GB 73.1 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.5/2.5 GB 73.1 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.5/2.5 GB 73.1 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.5/2.5 GB 73.1 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.5/2.5 GB 73.1 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 2.5/2.5 GB 22.9 MB/s eta 0:00:00\n",
      "Downloading https://download.pytorch.org/whl/cu126/torchaudio-2.6.0%2Bcu126-cp313-cp313-win_amd64.whl (4.2 MB)\n",
      "   ---------------------------------------- 0.0/4.2 MB ? eta -:--:--\n",
      "   ---------------------------------------- 4.2/4.2 MB 56.6 MB/s eta 0:00:00\n",
      "Downloading https://download.pytorch.org/whl/pillow-11.0.0-cp313-cp313-win_amd64.whl (2.6 MB)\n",
      "   ---------------------------------------- 0.0/2.6 MB ? eta -:--:--\n",
      "   ---------------------------------------- 2.6/2.6 MB 52.8 MB/s eta 0:00:00\n",
      "Installing collected packages: pillow, torch, torchvision, torchaudio\n",
      "  Attempting uninstall: torch\n",
      "    Found existing installation: torch 2.6.0\n",
      "    Uninstalling torch-2.6.0:\n",
      "      Successfully uninstalled torch-2.6.0\n",
      "Successfully installed pillow-11.0.0 torch-2.6.0+cu126 torchaudio-2.6.0+cu126 torchvision-0.21.0+cu126\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  WARNING: The scripts torchfrtrace.exe and torchrun.exe are installed in 'c:\\Users\\haris\\AppData\\Local\\Programs\\Python\\Python313\\Scripts' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\n",
      "  WARNING: Failed to remove contents in a temporary directory 'C:\\Users\\haris\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\~orch'.\n",
      "  You can safely remove it manually.\n",
      "\n",
      "[notice] A new release of pip is available: 24.3.1 -> 25.0.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu126 \n",
    "#Run if you have gpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from collections import Counter\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from transformers import BertForMaskedLM, BertConfig, AdamW, get_linear_schedule_with_warmup\n",
    "from torch.amp import autocast, GradScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tqdm import tqdm\n",
    "import random\n",
    "import os\n",
    "import json\n",
    "import math\n",
    "\n",
    "\n",
    "save_path = \"saved_models/viral_bert\"\n",
    "os.makedirs(save_path, exist_ok=True)  \n",
    "\n",
    "def save_model(model, epoch):\n",
    "    \"\"\"Save model checkpoint.\"\"\"\n",
    "    torch.save(model.state_dict(), f\"{save_path}/bert_epoch_{epoch}.pt\")\n",
    "    print(f\"Model saved at epoch {epoch}\")\n",
    "\n",
    "def load_model(model, epoch):\n",
    "    \"\"\"Load model checkpoint.\"\"\"\n",
    "    model.load_state_dict(torch.load(f\"{save_path}/bert_epoch_{epoch}.pt\"))\n",
    "    model.to(device)\n",
    "    print(f\"Loaded model from epoch {epoch}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 5606 genome sequences.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Load CSV file\n",
    "def load_genomes_from_csv(csv_file):\n",
    "    df = pd.read_csv(csv_file)\n",
    "    sequences = df[\"Genomic Sequence\"].dropna().tolist()  \n",
    "    return sequences\n",
    "\n",
    "# Example Usage\n",
    "csv_file = \"../Datasets/coronaviridae_sequences_with_genomes_final.csv\"\n",
    "genome_sequences = load_genomes_from_csv(csv_file)\n",
    "print(f\"Loaded {len(genome_sequences)} genome sequences.\")\n",
    "\n",
    "def kmer_tokenizer(sequence, k=6):\n",
    "    \"\"\"Generate k-mers from a sequence.\"\"\"\n",
    "    return [sequence[i:i+k] for i in range(len(sequence) - k + 1)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size: 26609\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Build Vocabulary\n",
    "def build_kmer_vocab(sequences, k=6, vocab_size=30000):\n",
    "    kmer_counts = Counter()\n",
    "    for seq in sequences:\n",
    "        kmer_counts.update(kmer_tokenizer(seq, k))\n",
    "    most_common_kmers = [kmer for kmer, _ in kmer_counts.most_common(vocab_size - 2)]\n",
    "    \n",
    "    vocab = {\"[PAD]\": 0, \"[MASK]\": 1}\n",
    "    vocab.update({kmer: i+2 for i, kmer in enumerate(most_common_kmers)})\n",
    "    return vocab\n",
    "\n",
    "vocab = build_kmer_vocab(genome_sequences, k=6, vocab_size=30000)\n",
    "print(f\"Vocabulary size: {len(vocab)}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total windows: 634036\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Encode Sequences\n",
    "def encode_sequence(sequence, vocab, k=6):\n",
    "    kmers = kmer_tokenizer(sequence, k)\n",
    "    return [vocab.get(kmer, 0) for kmer in kmers]\n",
    "\n",
    "encoded_sequences = [encode_sequence(seq, vocab) for seq in genome_sequences]\n",
    "\n",
    "# Create Overlapping Windows\n",
    "def create_overlapping_windows(sequence, window_size=512, stride=256):\n",
    "    windows = [sequence[i:i+window_size] for i in range(0, len(sequence), stride)]\n",
    "    if len(windows[-1]) < window_size:\n",
    "        windows[-1] += [0] * (window_size - len(windows[-1]))  # Pad last window\n",
    "    return windows\n",
    "\n",
    "windowed_sequences = [create_overlapping_windows(seq) for seq in encoded_sequences]\n",
    "flattened_windows = [win for seq in windowed_sequences for win in seq]\n",
    "print(f\"Total windows: {len(flattened_windows)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train size: 513568, Validation size: 57064, Test size: 63404\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Train/Validation/Test Split\n",
    "train_data, test_data = train_test_split(flattened_windows, test_size=0.1, random_state=42)\n",
    "train_data, val_data = train_test_split(train_data, test_size=0.1, random_state=42)\n",
    "\n",
    "print(f\"Train size: {len(train_data)}, Validation size: {len(val_data)}, Test size: {len(test_data)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Dataset Class\n",
    "class GenomicDataset(Dataset):\n",
    "    def __init__(self, sequences):\n",
    "        self.sequences = sequences\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.sequences)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        input_ids = torch.tensor(self.sequences[idx])\n",
    "        return input_ids\n",
    "\n",
    "def collate_fn(batch):\n",
    "    input_ids = [torch.tensor(seq) for seq in batch]\n",
    "    input_ids = torch.nn.utils.rnn.pad_sequence(input_ids, batch_first=True, padding_value=vocab[\"[PAD]\"])\n",
    "    attention_mask = (input_ids != vocab[\"[PAD]\"]).long()\n",
    "    return input_ids, attention_mask\n",
    "\n",
    "train_dataset = GenomicDataset(train_data)\n",
    "val_dataset = GenomicDataset(val_data)\n",
    "test_dataset = GenomicDataset(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "batch_size = 8\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, collate_fn=collate_fn)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, collate_fn=collate_fn)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, collate_fn=collate_fn)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "1\n",
      "NVIDIA GeForce RTX 3070\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(torch.cuda.is_available()) \n",
    "print(torch.cuda.device_count())  # Should print the number of GPUs, e.g., 1\n",
    "print(torch.cuda.get_device_name(0))  # Should print the name of your GPU (e.g., RTX 3070)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/5:   0%|          | 0/64196 [00:00<?, ?it/s]C:\\Users\\haris\\AppData\\Local\\Temp\\ipykernel_20772\\3346798437.py:14: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  input_ids = [torch.tensor(seq) for seq in batch]\n",
      "Epoch 1/5: 100%|██████████| 64196/64196 [1:24:26<00:00, 12.67it/s, loss=2.01e-7] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 | Avg Training Loss: 0.0465\n",
      "Epoch 1 | Validation Loss: 0.0003\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/5: 100%|██████████| 64196/64196 [1:24:05<00:00, 12.72it/s, loss=1.16e-9] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 | Avg Training Loss: 0.0003\n",
      "Epoch 2 | Validation Loss: 0.0001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/5: 100%|██████████| 64196/64196 [1:23:38<00:00, 12.79it/s, loss=5.82e-11]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 | Avg Training Loss: 0.0001\n",
      "Epoch 3 | Validation Loss: 0.0001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/5: 100%|██████████| 64196/64196 [1:23:38<00:00, 12.79it/s, loss=0]       \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 | Avg Training Loss: 0.0000\n",
      "Epoch 4 | Validation Loss: 0.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/5: 100%|██████████| 64196/64196 [1:24:24<00:00, 12.68it/s, loss=6.69e-10]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5 | Avg Training Loss: 0.0000\n",
      "Epoch 5 | Validation Loss: 0.0000\n",
      "Test Loss: 0.0000\n"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "# Model Configuration\n",
    "config = BertConfig(vocab_size=len(vocab), hidden_size=288, num_hidden_layers=4, num_attention_heads=6, max_position_embeddings=512)\n",
    "model = BertForMaskedLM(config)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "# Regularization and Optimizer\n",
    "optimizer = AdamW(model.parameters(), lr=5e-5, weight_decay=0.01)\n",
    "total_steps = len(train_dataloader) * 5\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=0, num_training_steps=total_steps)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Training Loop\n",
    "epochs = 5\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    progress_bar = tqdm(train_dataloader, desc=f\"Epoch {epoch+1}/{epochs}\")\n",
    "    \n",
    "    for input_ids, attention_mask in progress_bar:\n",
    "        input_ids, attention_mask = input_ids.to(device), attention_mask.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        outputs = model(input_ids, attention_mask=attention_mask, labels=input_ids)\n",
    "        loss = outputs.loss\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "        progress_bar.set_postfix(loss=loss.item())\n",
    "    \n",
    "    avg_loss = total_loss / len(train_dataloader)\n",
    "    print(f\"Epoch {epoch+1} | Avg Training Loss: {avg_loss:.4f}\")\n",
    "    \n",
    "    # Validation\n",
    "    model.eval()\n",
    "    val_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for input_ids, attention_mask in val_dataloader:\n",
    "            input_ids, attention_mask = input_ids.to(device), attention_mask.to(device)\n",
    "            outputs = model(input_ids, attention_mask=attention_mask, labels=input_ids)\n",
    "            val_loss += outputs.loss.item()\n",
    "    \n",
    "    avg_val_loss = val_loss / len(val_dataloader)\n",
    "    print(f\"Epoch {epoch+1} | Validation Loss: {avg_val_loss:.4f}\")\n",
    "\n",
    "# Evaluate on Test Set\n",
    "model.eval()\n",
    "test_loss = 0\n",
    "with torch.no_grad():\n",
    "    for input_ids, attention_mask in test_dataloader:\n",
    "        input_ids, attention_mask = input_ids.to(device), attention_mask.to(device)\n",
    "        outputs = model(input_ids, attention_mask=attention_mask, labels=input_ids)\n",
    "        test_loss += outputs.loss.item()\n",
    "\n",
    "avg_test_loss = test_loss / len(test_dataloader)\n",
    "print(f\"Test Loss: {avg_test_loss:.4f}\")\n",
    "\n",
    "# Save Model\n",
    "model.save_pretrained(\"viral_bert_final_gen_2\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\haris\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\transformers\\optimization.py:640: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "Epoch 1/5:   0%|          | 0/79255 [00:00<?, ?it/s]C:\\Users\\haris\\AppData\\Local\\Temp\\ipykernel_63104\\3552041536.py:26: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  input_ids = [torch.tensor(seq) for seq in input_ids]\n",
      "C:\\Users\\haris\\AppData\\Local\\Temp\\ipykernel_63104\\3552041536.py:27: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  labels = [torch.tensor(lbl) for lbl in labels]\n",
      "Epoch 1/5: 100%|██████████| 79255/79255 [1:56:29<00:00, 11.34it/s, loss=0.0725]     \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 | Avg Loss: 0.6779\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/5: 100%|██████████| 79255/79255 [1:05:39<00:00, 20.12it/s, loss=0.0192] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 | Avg Loss: 0.0374\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/5: 100%|██████████| 79255/79255 [1:05:47<00:00, 20.08it/s, loss=0.0251] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 | Avg Loss: 0.0223\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/5: 100%|██████████| 79255/79255 [1:05:50<00:00, 20.06it/s, loss=0.0148] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 | Avg Loss: 0.0172\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/5: 100%|██████████| 79255/79255 [1:05:53<00:00, 20.05it/s, loss=0.00798]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5 | Avg Loss: 0.0147\n"
     ]
    }
   ],
   "source": [
    "# Model Configuration\n",
    "config = BertConfig(\n",
    "    vocab_size=len(vocab),\n",
    "    hidden_size=288,\n",
    "    num_hidden_layers=4,\n",
    "    num_attention_heads=6,  # 288 is divisible by 6\n",
    "    max_position_embeddings=512\n",
    ")\n",
    "model = BertForMaskedLM(config)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "# Optimizer and Scheduler\n",
    "optimizer = AdamW(model.parameters(), lr=5e-5, weight_decay=0.01)\n",
    "total_steps = len(dataloader) * 5  # 5 epochs\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=0, num_training_steps=total_steps)\n",
    "# FP16 Mixed Precision Training\n",
    "scaler = GradScaler()\n",
    "\n",
    "epochs = 5\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    progress_bar = tqdm(dataloader, desc=f\"Epoch {epoch+1}/{epochs}\")\n",
    "\n",
    "    for input_ids, attention_mask, labels in progress_bar:\n",
    "        input_ids, attention_mask, labels = input_ids.to(device), attention_mask.to(device), labels.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        with autocast(device_type='cuda'):\n",
    "            outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n",
    "            loss = outputs.loss\n",
    "            \n",
    "        scaler.scale(loss).backward()\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "        scheduler.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "        progress_bar.set_postfix(loss=loss.item())\n",
    "\n",
    "    avg_loss = total_loss / len(dataloader)\n",
    "    print(f\"Epoch {epoch+1} | Avg Loss: {avg_loss:.4f}\")\n",
    "    \n",
    "    # Save Model\n",
    "    model.save_pretrained(f\"viral_bert_epoch_{epoch+1}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation set size: 1122\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Assuming `encoded_sequences` contains your preprocessed sequences\n",
    "train_seqs, val_seqs = train_test_split(encoded_sequences, test_size=0.2, random_state=42)\n",
    "\n",
    "# Pad sequences to max length\n",
    "max_length = max(len(seq) for seq in encoded_sequences)\n",
    "val_seqs_padded = [seq + [0] * (max_length - len(seq)) for seq in val_seqs]\n",
    "\n",
    "# Convert to tensors\n",
    "val_input_ids = torch.tensor(val_seqs_padded)\n",
    "\n",
    "# Create DataLoader for validation\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "batch_size = 8\n",
    "val_dataset = TensorDataset(val_input_ids)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, collate_fn=collate_fn)\n",
    "\n",
    "print(f\"Validation set size: {len(val_input_ids)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 10 genome sequences.\n",
      "Vocabulary size: 3754\n"
     ]
    }
   ],
   "source": [
    "# Load CSV file\n",
    "def load_genomes_from_csv(csv_file):\n",
    "    df = pd.read_csv(csv_file)\n",
    "    sequences = df[\"Genomic Sequence\"].dropna().tolist()  # Ensure no NaN values\n",
    "    return sequences\n",
    "\n",
    "# Example Usage\n",
    "csv_file = \"../Datasets/coronaviridae_test_run.csv\"\n",
    "genome_sequences = load_genomes_from_csv(csv_file)\n",
    "print(f\"Loaded {len(genome_sequences)} genome sequences.\")\n",
    "\n",
    "def kmer_tokenizer(sequence, k=6):\n",
    "    \"\"\"Generate k-mers from a sequence.\"\"\"\n",
    "    return [sequence[i:i+k] for i in range(len(sequence) - k + 1)]\n",
    "\n",
    "vocab = build_kmer_vocab(genome_sequences, k=6, vocab_size=30000)\n",
    "print(f\"Vocabulary size: {len(vocab)}\")\n",
    "\n",
    "\n",
    "encoded_sequences = [encode_sequence(seq, vocab) for seq in genome_sequences]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 1/1 [00:00<00:00,  4.91it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 0.0416\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import torch\n",
    "\n",
    "# Assuming `encoded_sequences` contains your preprocessed sequences\n",
    "train_seqs, val_seqs = train_test_split(encoded_sequences, test_size=0.2, random_state=42)\n",
    "\n",
    "# Pad sequences to max length\n",
    "max_length = 512  # Make sure this matches your model's max_position_embeddings\n",
    "train_seqs_padded = [seq + [0] * (max_length - len(seq)) if len(seq) < max_length else seq[:max_length] for seq in train_seqs]\n",
    "val_seqs_padded = [seq + [0] * (max_length - len(seq)) if len(seq) < max_length else seq[:max_length] for seq in val_seqs]\n",
    "\n",
    "# Convert to tensors\n",
    "train_input_ids = torch.tensor(train_seqs_padded)\n",
    "val_input_ids = torch.tensor(val_seqs_padded)\n",
    "\n",
    "# Labels for MLM are the same as input_ids, but we will mask out padding tokens\n",
    "train_labels = train_input_ids.clone()\n",
    "val_labels = val_input_ids.clone()\n",
    "\n",
    "# Create DataLoader for validation\n",
    "batch_size = 8\n",
    "val_dataset = TensorDataset(val_input_ids, val_labels)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# Evaluate\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "model.eval()  # Set model to evaluation mode\n",
    "\n",
    "val_losses = []\n",
    "with torch.no_grad():\n",
    "    for batch in tqdm(val_dataloader, desc=\"Evaluating\"):\n",
    "        input_ids, labels = batch\n",
    "        input_ids, labels = input_ids.to(device), labels.to(device)\n",
    "\n",
    "        # Forward pass\n",
    "        with autocast(device_type='cuda'):\n",
    "            outputs = model(input_ids, labels=labels)\n",
    "            loss = outputs.loss\n",
    "        \n",
    "        val_losses.append(loss.item())\n",
    "\n",
    "# Calculate average validation loss\n",
    "avg_val_loss = sum(val_losses) / len(val_losses)\n",
    "print(f\"Validation Loss: {avg_val_loss:.4f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
